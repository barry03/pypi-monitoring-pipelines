# Pipeline de Monitoring des T√©l√©chargements PyPi

## Pr√©sentation

Ce projet met en place un pipeline ETL pour monitorer les t√©l√©chargements PyPi sur BigQuery. Il comprend :
- **Extraction** des donn√©es depuis `bigquery-public-data.pypi.file_downloads`
- **Transformation** des donn√©es (formatage, ajout de m√©triques comme `is_gzipped`)
- **Chargement** des donn√©es sur **Google Cloud Storage (GCS)**
- **Orchestration** du pipeline avec **Airflow (Cloud Composer)**
- **Visualisation** avec **Looker Studio et Streamlit**
- **Cr√©ation de vues BigQuery** pour des analyses optimis√©es

## **Architecture du projet**

Voici un aper√ßu de l'architecture du pipeline ETL : Le pipeline suit une architecture en plusieurs √©tapes allant de l‚Äôextraction des donn√©es brutes jusqu'√† leur visualisation dans des dashboards interactifs.

<img src="docs/archi.png" alt="Architecture ETL PyPi" width="600" heigth="450">

## **Installation et Configuration**
### 1 - **Pr√©requis**
Assurez-vous d'avoir :
- **Python 3.11+** et `pip` install√©s
- **Google Cloud SDK** (`gcloud`) configur√© avec un projet actif
- **BigQuery API, Cloud Storage API et Cloud Composer activ√©s**
- **Un bucket GCS** pour stocker les fichiers transform√©s
- **Un environnement Cloud Composer (Airflow)** configur√©

### **2 - Cloner le projet et installer les d√©pendances**
```bash
git clone https://github.com/votre-repo/pypi_project.git
cd pypi_project
pip install -r requirements.txt
```
### **3 - D√©ploiement**
Ce projet peut √™tre d√©ploy√© de deux mani√®res : 1. Avec le script `deploy.sh` (d√©ploiement automatiquement sur GCS & BigQuery)
2. **Avec Docker** (sous forme d'image conteneuris√©e)

**Option 1:** D√©ploiement Automatis√© avec `deploy.sh`
Ce script automatise tout le processus:
- Envoi du code source dans GCS
- D√©ploiement du DAG sur Cloud Composer
- Cr√©ation/Mise √† jour des vues BigQuery
- Lancement du pipeline en local
```bash
chmod +x deploy.sh  # Rendre le script ex√©cutable
./deploy.sh
```
NB: Assurez-vous d‚Äôavoir configur√© `gcloud` et d‚Äô√™tre connect√© √† votre projet GCP avant d‚Äôex√©cuter ce script.

**Option 2 :** D√©ploiement avec Docker
Une image Docker a √©t√© cr√©√©e pour √©x√©cuter ce projet dans un environnement conteneuris√©.

**√âtapes de d√©ploiement :**
1. Construire l'image Docker:
```bash
docker build -t pypi-monitoring-pipeline .
```
2. Ex√©cuter le conteneur et lancer le pipeline :
```bash
docker run --rm pypi-monitoring-pipeline
```
3. Pousser l‚Äôimage sur Google Container Registry (GCR) :
```bash
gcloud auth configure-docker
docker tag pypi-monitoring-pipeline gcr.io/<PROJECT_ID>/pypi-monitoring-pipeline
docker push gcr.io/<PROJECT_ID>/pypi-monitoring-pipeline
```
**NB:** Remplacer <PROJECT_ID> par l'ID de votre projet

**D√©ploiement du DAG Airflow (Manuel)**
Si vous souhaitez ajouter ou mettre √† jour manuellement le DAG sur Cloud Composer :
```bash
gcloud storage cp dags/dag_pypi.py gs://<YOUR_COMPOSER_BUCKET>/dags/
```
- Acc√©dez √† Airflow Web UI depuis Cloud Composer.
- Activez le DAG pypi_data_pipeline
- Lancez une ex√©cution manuelle.

### **4 - Ex√©cution du pipeline en local**
  ```bash
  python src/main.py
  ```

### **5 - Utilisation des Vues BigQuery**
Ce projet utilise les tables brutes **BigQuery Public Datasets**, mais pour am√©liorer les performances, des **vues BigQuery** sont disponibles.

**- Pourquoi utiliser les vues BigQuery ?**

- **Moins de volume de donn√©es extraites** ‚Üí R√©duction des co√ªts et des temps d‚Äôex√©cution.
- **Donn√©es d√©j√† filtr√©es et pr√©-transform√©es** ‚Üí Moins de transformations dans le pipeline.
- **Plus rapide pour l'analyse et le dashboard**.

**- Vues disponibles :**
| Nom de la vue | Description |
| ------ | ------ |
| `pypi_views.fifth_downloads` | Contient uniquement les t√©l√©chargements des `15 derniers jours`|
| `pypi_views.downloads_by_day_country`   | Agr√®ge le nombre de t√©l√©chargements par jour et par pays|
| `pypi_views.gzipped_downloads` | Filtre uniquement les fichiers compress√©s (`.gz`)|
| `pypi_views.download_intervals` | Analyser la fr√©quence des t√©l√©chargements des projets sur PyPI|


**- Comment utiliser une vue dans le pipeline ?**
Si vous souhaitez utiliser une **vue BigQuery** au lieu des tables brutes, modifiez `extract.py` en rempla√ßant la requ√™te SQL par :
```python
query = "SELECT * FROM `western-watch-418016.pypi_views.fifth_downloads`"
```

### **6 - Acc√®s aux r√©sultats**
- Les fichiers transform√©s sont disponibles sur **Cloud Storage** (gs://<YOUR_BUCKET_NAME>/data/)
- Les vues sont accessibles sur BigQuery (western-watch-418016.pypi_views).
- Les dashboards sont accessibles sur Looker Studio ou Streamlit.

### **7 - Visualisation des Dashboards**

Les dashboards sont accessibles via **Looker Studio** (via BigQuery) ou en local avec **Streamlit**.

** - Ex√©cution du dashboard avec Streamlit**
Si vous souhaitez visualiser les donn√©es en local, ex√©cutez :

```bash
streamlit run src/dashboard.py
```
- Pr√©requis : Assurez-vous d‚Äôavoir Streamlit install√© dans votre environnement
```bash
pip install streamlit
```

### **8 - Contact**
Si vous avez des questions, vous pouvez me contacter :
- üìß Email : barrydjoulde15@gmail.com
- üîó LinkedIn : https://www.linkedin.com/in/djould%C3%A9-barry-24868a187
- üìù Issues : Ouvrez une issue sur [le repo GitLab](https://gitlab.com/barrydjoulde/pypi-monitoring-pipeline)





