# Pipeline de Monitoring des T√©l√©chargements PyPi

## Pr√©sentation

Ce projet met en place un pipeline ETL pour monitorer les t√©l√©chargements PyPi sur BigQuery. Il comprend :
- **Extraction** des donn√©es depuis `bigquery-public-data.pypi.file_downloads`
- **Transformation** des donn√©es (formatage, ajout de m√©triques comme `is_gzipped`)
- **Chargement** des donn√©es sur **Google Cloud Storage (GCS)**
- **Orchestration** du pipeline avec **Airflow (Cloud Composer)**
- **Visualisation** avec **Looker Studio et Streamlit**
- **Cr√©ation de vues BigQuery** pour des analyses optimis√©es

## **Architecture du projet**

Voici un aper√ßu de l'architecture du pipeline ETL : Le pipeline suit une architecture en plusieurs √©tapes allant de l‚Äôextraction des donn√©es brutes jusqu'√† leur visualisation dans des dashboards interactifs.

<img src="docs/arch.png" alt="Architecture ETL PyPi" width="600">

## **Installation et Configuration**
### 1 - **Pr√©requis**
Assurez-vous d'avoir :
- **Python 3.11+** et `pip` install√©s
- **Google Cloud SDK** (`gcloud`) configur√© avec un projet actif
- **BigQuery API, Cloud Storage API et Cloud Composer activ√©s**
- **Un bucket GCS** pour stocker les fichiers transform√©s
- **Un environnement Cloud Composer (Airflow)** configur√©

### **2 - Cloner le projet et installer les d√©pendances**
```bash
git clone https://github.com/votre-repo/pypi_project.git
cd pypi_project
pip install -r requirements.txt
```
### **3 - D√©ploiement**
#### D√©ploiement du DAG Airflow
- Placez le fichier `dag_pypi.py` dans le r√©pertoire `dags/` du bucket Cloud Composer.
- Synchronisez les fichiers avec :
  ```bash
  gcloud storage cp dags/dag_pypi.py gs://<YOUR_COMPOSER_BUCKET>/dags/
  ```

### **4 - Ex√©cution du pipeline en local**
  ```bash
  python src/main.py
  ```

### **5 - Utilisation des Vues BigQuery**
Ce projet utilise les tables brutes **BigQuery Public Datasets**, mais pour am√©liorer les performances, des **vues BigQuery** sont disponibles.

**- Pourquoi utiliser les vues BigQuery ?**

- **Moins de volume de donn√©es extraites** ‚Üí R√©duction des co√ªts et des temps d‚Äôex√©cution.
- **Donn√©es d√©j√† filtr√©es et pr√©-transform√©es** ‚Üí Moins de transformations dans le pipeline.
- **Plus rapide pour l'analyse et le dashboard**.

**- Vues disponibles :**
| Nom de la vue | Description |
| ------ | ------ |
| `pypi_views.50_downloads` | Contient uniquement les t√©l√©chargements des 15 derniers jours|
| `pypi_views.file_downloads_extended`   | Ajoute la dur√©e entre t√©l√©chargements et l‚Äôindicateur `is_gzipped`|
| `pypi_views.gzipped_files` | Filtre uniquement les fichiers compress√©s (`.gz`)|


**- Comment utiliser une vue dans le pipeline ?**
Si vous souhaitez utiliser une **vue BigQuery** au lieu des tables brutes, modifiez `extract.py` en rempla√ßant la requ√™te SQL par :
```python
query = "SELECT * FROM `western-watch-418016.pypi_views.fifth_downloads`"
```

### **6 - Acc√®s aux r√©sultats**
- Les fichiers transform√©s sont disponibles sur **Cloud Storage** (gs://<YOUR_BUCKET_NAME>/data/)
- Les vues sont accessibles sur BigQuery (western-watch-418016.pypi_views).
- Les dashboards sont accessibles sur Looker Studio ou Streamlit.

### **7 - Visualisation des Dashboards**

Les dashboards sont accessibles via **Looker Studio** (via BigQuery) ou en local avec **Streamlit**.

** - Ex√©cution du dashboard avec Streamlit**
Si vous souhaitez visualiser les donn√©es en local, ex√©cutez :

```bash
streamlit run src/dashboard.py
```
- Pr√©requis : Assurez-vous d‚Äôavoir Streamlit install√© dans votre environnement
```bash
pip install streamlit
```

### **8 Contact**
Si vous avez des questions, vous pouvez me contacter :
- üìß Email : barrydjoulde15@gmail.com
- üîó LinkedIn : https://www.linkedin.com/in/djould%C3%A9-barry-24868a187
- üìù Issues : Ouvrez une issue sur [le repo GitLab](https://gitlab.com/barrydjoulde/pypi-monitoring-pipeline)





